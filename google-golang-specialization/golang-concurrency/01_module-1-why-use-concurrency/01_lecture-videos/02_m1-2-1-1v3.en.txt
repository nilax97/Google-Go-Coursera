Module one, why use concurrency? Topic 2.1: Concurrent vs Parallel. So, I've been talking about parallel execution. Now, I'm going to use this term concurrent. Parallel and concurrent are related ideas, but concurrent program is really what we're going to do and they're slightly different, this parallel verses concurrent. Concurrent execution is not necessarily executing at the same time. It could be, but it's not necessarily. So, concurrent. You can call two tasks concurrent, and by task I mean, let's think of a program for a second, we'll get more specific about what a task is later. But you say two tasks are concurrent if the start time and the end time of the tasks overlap. So, that range of time between start and end for these two tasks, if they overlap, then you say that is concurrent. That does not mean that they're literally executing at the same time, where with parallel, they have to literally be executing at the same time. So, if you look at the picture, on the left you got task one, task two, there's timeline. Time is increasing down. So, there's a start, if you look at task one it starts early, then it executes it where you see that black line, that black vertical line under task one, it's executing, okay? Then there's that blue dash line where task one stops executing, tasks two start executing. Then task two executes, it starts it ends, and then when it's done, task one continues to execute until its end. So, notice it starts one and end one for task one and start two, end two for task two. Now these regions the time between the start and the end of task one, starting end of test two on the left, they overlap in time. So, task two start and end time overlaps task one won't start in time. So, you would say these two are executing concurrently. Now they are not executing in parallel because if you look at any one time instance, either task one is executing or task two is executing. So, they're not parallel but they are concurrent. Now on the right-hand side, we're showing actual concurrency. Sorry, actual parallelism. So, you got the two tasks and again, they have the different start and end times that overlap, so they're definitely concurrent, but also you can see in that middle region at a time when task two's executing, task one is actually executing at the same time. So, on the right we're seeing actual parallel execution,where on the left we're seeing concurrent execution. There times overlap but they're not actually executing at the same time. Now one thing to note is that on the left at the concurrent execution, the completion time for both tasks is longer than on the right and the completion time when they're working in parallel. So parallelism gives you that, gives you a better, improved completion time, we already talked about that. So, one might ask why do concurrent execution if you're not executing at the same time, if it's not actually parallel? Maybe concurrent execution is not beneficial, right? I mean why not just do task one and then do task two right after it, forget the concurrent execution, and the end time would still be the same. Well there is a benefit and we'll talk about that. Okay so, just again to reinforce this difference between concurrent and parallel, parallel tasks must be executed on different hardware. In order to execute in parallel, you need different hardware, different cores typically. Concurrent tasks may be executed on the same hardware but may not. You could have them on the one processor core and since they're actually alternating, like we saw in the last slide, you can execute them on one piece of hardware, one core, and you can still say this is concurrent execution but only one task executed at a time. So, this process of mapping the task to hardware is not directly controlled by the programmer. So, when I say mapping the task to the hardware, you've got these tasks, task one, task two, that you need to perform on some hardware, core one core two. And determining which core is performing which task, that's what I'm calling a hardware mapping. That is not directly controlled by the programmer, and certainly not in Go. Now there exists languages where the programmer directly controls that sort of thing. Those are generally considered, I'll call them exotic, they're not commonly used, put it like that. But that's a complicated thing and it's not part of Go, and it's not part of most languages either. So, most of the time, in a language like Go, when you program, when you do concurrent programming, the programmer in doing their concurrent programming, they're determining which tasks can be executed in parallel. Also, there terming a few other things like what kind of communication there is between the tasks, maybe one sends data to the other, are also synchronization events, we'll get into all these details later. But maybe, for instance, this test has to do some stuff before this test does it's other stuff, so they have to synchronize on a certain time. This must be finished before this time, this can finish after the time, something like that. We'll talk about that, but that's what the programmer does. A programmer describes what can be executed in parallel, not what will be executed in parallel. What will be executed in parallel depends on how the tasks are mapped to hardware. Now, how the tests are mapped to hardware, that is not directly in the control of the programmer, that's left to the operating system, also the Go run time scheduler that does part of the test two we'll talk about that too. That's part of the Go language actually, it gets compiled into your code and it handles some of that too. But concurrent programming is where the programmer defines the possible wconcurrency, What can be done in parallel, what can't be. But what actually is done in parallel and what is not depends on the mapping to hardware, and that's not under the control of the programmer. Now, let's say you are doing this, say you have one core. So you're doing concurrent programming but you've only got one core, so you can't actually get parallelism. So, one might say, look, why bother? Why go to all that effort of concurrent programming, if I'm only going to get concurrency anyway everything's happening one instruction at a time, these tasks can't actually run together at the same time. So, even though you can't get parallelism, maybe can't get parallelism cause you don't have concurrent multiple processes, multiple cores, you can still get significant performance improvement just from using concurrency. So, the reason why this happens is because typically, tasks have to periodically wait for some event, some slow event. Now, there's many of these slow events, typical input output things. So the idea is that in a machine there's a CPU, a processor, and that runs fast. But when the processor has to communicate with other things that are other chips on the same board, that is slow. So for instance say I got my processor and it needs to talk to some memory card that is far away on the board. When I say far away, I mean this far, but that's far, in terms of signal transmission, that's far. So he's got to communicate with a memory to grab some data from memory. And memories are slow so you typically have to wait for that. Maybe it wants to send data on the network, so it's got to communicate with this Ethernet card, or Wi-Fi card. And the Wi-Fi card is slow, it takes a certain amount of time to hand over the communication to receive or send. So anytime the CPU wants to do- or maybe you want to put something on screen, you've got to go to a video card. So the CPU is communicating with a video card and that has a certain delay. So when you have these IO activities where the CPU is communicating with something else on the board but separate from it, those are slow and often the code has to wait until those things are done. So for instance here, got this really simple instruction x equals y plus z. So in order to do this instruction, you've got to read y and z from memory, and then you can compute the addition and then you can put the result back in memory and x. These memory accesses reading y and z from memory can take over 100 clock cycles just waiting on memory to grab the data from memory. Now the add instruction itself takes you one clock cycle. But you might be waiting a 100 clock cycles just to read the data from memory. So when these type of delays, these pauses happen, they don't happen often. Hopefully, it depends on how your code's written. Hopefully it don't happen too often, but they can. Depending on how you access memory and memory access patterns and that sort of thing. When they happen, you would rather not have your processor just waiting there for 100 clock cycles, waiting for memory. It would be better if the task that's doing this x plus y, x equals y plus z, that task it has to wait for 100 cycles but in the meantime you would like to have that processor core executing some useful code. So if you have multiple tasks that all can execute when task 1 is waiting for memory, task 2 can execute, and then when task 2 maybe it hits a memory access, then task 3 can execute while the other guys are waiting. So that's called hiding latency. That's generally what you refer to with hiding latency. So there's a latency, like a wait time latency. So say I'm going to memory I need to wait 100 cycles. You can hide that latency by just taking another task that's ready to run and execute that while you're waiting on the first event. So even without having parallel hardware, having multiple core essay, just writing concurrent code can give you this significant advantage in performance because your program no longer has to wait on IO input-output accesses, whatever type of input-output accesses. No longer has to wait it can just run and our program's ready. And so things can still get done faster. So concurrent programming is useful even without parallel hardware. Now what I'm showing here is a picture that hardware mapping, I was talking about this hardware mapping concept. In the top I'm showing what parallel execution might look like, bottom I'm showing what concurrent execution might look like. So the idea here is, look at the parallel execution one first, you've got task 1, task 2. Imagine these are blocks of code written in whatever language, Go or whatever it is. And at the bottom you've got core 1 and core 2, so these are two cores in a 2-core system. This hardware mapping is a task of saying, this task runs on this core. So you can see with the parallel execution I'm mapping these two tasks to run on two different cores, task 1 and core 1, task 2 and core 2 so they can execute in parallel at the same time. In the bottom you do a different hardware mapping, you say, "Look I got two tasks and they are both performing, executing on the same core." That would be concurrent execution. But a key thing here is that this hardware mapping task from the task to the core, that is not directly under the control of the programmer in Go or any other substandard language. So, just to give an example why that's not- you might actually argue, "Well, look why not? Why not let the programmer do it?" That is a hard problem, it is a difficult problem and programmers generally don't want to do that. It would slow you down in writing your code. It makes your coding a lot harder if you actually perform the hardware mapping. There's a lot of research in doing this hardware mapping and automatic techniques for doing it, it's a hard task. The reason why is because these hardware architectures can be complicated and there are a lot of things that need to be considered when you're deciding which core is going to perform which task. And it would be a really big burden on the programmer to have to consider all these factors. And the program would have to be completely aware of the underlying hardware architecture. One good thing about traditional programming languages like Go Lang, but also Python C, whatever, is that when you're writing code in these languages you are not directly aware of the underlying hardware architecture. You don't know is this an I7 or is this an I3 or whatever. You don't know that, and you don't care. And you don't know exactly what the architecture looks like and you don't generally care. Now, to some extent you care a little. You might want to know, look I have at least this much memory. Maybe you care at a broad level. But the details of the hardware architecture, your coding is not dependent on that and you really don't want it to be because it is complicated. So what I've drawn here is a picture of a relatively simple arbitrary multi--core system. And this multi-core system has got these four cores, those blue cores around the edges. It's got four caches, local caches for each core which is extremely common. Actually this is a simplification of what you would expect to see but each one's got its local cache and then in the middle is a shared memory that all of them share and can read from and dump into the caches and so on. So when you're figuring out this hardware mapping saying, "Look, this task should be performed on this core this on this core." One big consideration is, where is the data? So what I mean by that is, if a particular core, core 1 let say. So core one is going to perform this particular task then it needs to access the data that the task uses. So where is the data located? The data, should you want it to be in the local cache for that core but say the data is in the shared memory or in the cache for another core or something like that. If the data is not where you want it, then if core 1- so for instance core 1's performing a task but all of it's data is on core 2's cache. Then, you've got to do these memory transfers. And remember the Von Neumann Bottleneck. Memory is slow. So doing transfers from memory to memory, during reads of memory can slow you down. So when you do this hardware mapping tests you've got to think, "Okay, where is the data? Which memories have the data and which don't?" And you basically want to put the task near the data. Also, what are the communication costs? So, maybe a cache to cache transfer is pretty fast because caches are fast, but if I could do cache to shared memory, shared memory is slower. So you would have to consider which communications are fast versus slow. So it can get to be a very complicated problem and I know that because I've seen plenty of research papers on that subject. It is a complicated problem it is not something that generally a programmer wants to have to consider. So it is intentionally left out of traditional languages like Go. So in Go we are doing concurrent programming so we can define which task can be performed in parallel, but which tasks are performed in parallel is left up to operating system Go Runtime something else. The programmer doesn't have to consider that. Thank you.