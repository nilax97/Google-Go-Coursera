1
00:00:00,000 --> 00:00:02,665
Module one, why use concurrency?

2
00:00:02,665 --> 00:00:05,960
Topic 2.1: Concurrent vs Parallel.

3
00:00:05,960 --> 00:00:10,015
So, I've been talking about parallel execution.

4
00:00:10,015 --> 00:00:12,775
Now, I'm going to use this term concurrent.

5
00:00:12,775 --> 00:00:15,105
Parallel and concurrent are related ideas,

6
00:00:15,105 --> 00:00:18,230
but concurrent program is really what we're going to do and they're slightly different,

7
00:00:18,230 --> 00:00:19,925
this parallel verses concurrent.

8
00:00:19,925 --> 00:00:24,595
Concurrent execution is not necessarily executing at the same time.

9
00:00:24,595 --> 00:00:28,300
It could be, but it's not necessarily.

10
00:00:28,300 --> 00:00:30,230
So, concurrent.

11
00:00:30,230 --> 00:00:33,555
You can call two tasks concurrent, and by task I mean,

12
00:00:33,555 --> 00:00:34,995
let's think of a program for a second,

13
00:00:34,995 --> 00:00:37,340
we'll get more specific about what a task is later.

14
00:00:37,340 --> 00:00:39,920
But you say two tasks are concurrent if

15
00:00:39,920 --> 00:00:43,330
the start time and the end time of the tasks overlap.

16
00:00:43,330 --> 00:00:46,980
So, that range of time between start and end for these two tasks,

17
00:00:46,980 --> 00:00:49,690
if they overlap, then you say that is concurrent.

18
00:00:49,690 --> 00:00:53,450
That does not mean that they're literally executing at the same time,

19
00:00:53,450 --> 00:00:57,440
where with parallel, they have to literally be executing at the same time.

20
00:00:57,440 --> 00:01:00,320
So, if you look at the picture, on the left you got task one,

21
00:01:00,320 --> 00:01:01,450
task two, there's timeline.

22
00:01:01,450 --> 00:01:03,940
Time is increasing down.

23
00:01:03,940 --> 00:01:06,235
So, there's a start,

24
00:01:06,235 --> 00:01:08,385
if you look at task one it starts early,

25
00:01:08,385 --> 00:01:11,030
then it executes it where you see that black line,

26
00:01:11,030 --> 00:01:14,205
that black vertical line under task one, it's executing, okay?

27
00:01:14,205 --> 00:01:18,010
Then there's that blue dash line where task one stops executing,

28
00:01:18,010 --> 00:01:19,770
tasks two start executing.

29
00:01:19,770 --> 00:01:21,630
Then task two executes,

30
00:01:21,630 --> 00:01:22,850
it starts it ends,

31
00:01:22,850 --> 00:01:24,615
and then when it's done,

32
00:01:24,615 --> 00:01:27,840
task one continues to execute until its end.

33
00:01:27,840 --> 00:01:31,990
So, notice it starts one and end one for task one and start two,

34
00:01:31,990 --> 00:01:33,280
end two for task two.

35
00:01:33,280 --> 00:01:36,930
Now these regions the time between the start and the end of task one,

36
00:01:36,930 --> 00:01:38,805
starting end of test two on the left,

37
00:01:38,805 --> 00:01:40,855
they overlap in time.

38
00:01:40,855 --> 00:01:44,900
So, task two start and end time overlaps task one won't start in time.

39
00:01:44,900 --> 00:01:47,695
So, you would say these two are executing concurrently.

40
00:01:47,695 --> 00:01:52,810
Now they are not executing in parallel because if you look at any one time instance,

41
00:01:52,810 --> 00:01:56,100
either task one is executing or task two is executing.

42
00:01:56,100 --> 00:01:58,805
So, they're not parallel but they are concurrent.

43
00:01:58,805 --> 00:02:00,769
Now on the right-hand side,

44
00:02:00,769 --> 00:02:02,604
we're showing actual concurrency.

45
00:02:02,604 --> 00:02:04,160
Sorry, actual parallelism.

46
00:02:04,160 --> 00:02:06,860
So, you got the two tasks and again,

47
00:02:06,860 --> 00:02:08,815
they have the different start and end times that overlap,

48
00:02:08,815 --> 00:02:10,195
so they're definitely concurrent,

49
00:02:10,195 --> 00:02:14,845
but also you can see in that middle region at a time when task two's executing,

50
00:02:14,845 --> 00:02:17,395
task one is actually executing at the same time.

51
00:02:17,395 --> 00:02:19,985
So, on the right we're seeing actual parallel

52
00:02:19,985 --> 00:02:23,375
execution,where on the left we're seeing concurrent execution.

53
00:02:23,375 --> 00:02:26,795
There times overlap but they're not actually executing at the same time.

54
00:02:26,795 --> 00:02:32,550
Now one thing to note is that on the left at the concurrent execution,

55
00:02:32,550 --> 00:02:35,380
the completion time for both tasks is

56
00:02:35,380 --> 00:02:39,085
longer than on the right and the completion time when they're working in parallel.

57
00:02:39,085 --> 00:02:40,920
So parallelism gives you that,

58
00:02:40,920 --> 00:02:44,330
gives you a better, improved completion time, we already talked about that.

59
00:02:44,330 --> 00:02:51,260
So, one might ask why do concurrent execution if you're not executing at the same time,

60
00:02:51,260 --> 00:02:52,985
if it's not actually parallel?

61
00:02:52,985 --> 00:02:56,170
Maybe concurrent execution is not beneficial, right?

62
00:02:56,170 --> 00:02:59,080
I mean why not just do task one and then do task two right after it,

63
00:02:59,080 --> 00:03:00,640
forget the concurrent execution,

64
00:03:00,640 --> 00:03:02,695
and the end time would still be the same.

65
00:03:02,695 --> 00:03:05,735
Well there is a benefit and we'll talk about that.

66
00:03:05,735 --> 00:03:10,265
Okay so, just again to reinforce this difference between concurrent and parallel,

67
00:03:10,265 --> 00:03:15,175
parallel tasks must be executed on different hardware.

68
00:03:15,175 --> 00:03:16,430
In order to execute in parallel,

69
00:03:16,430 --> 00:03:19,100
you need different hardware, different cores typically.

70
00:03:19,100 --> 00:03:23,925
Concurrent tasks may be executed on the same hardware but may not.

71
00:03:23,925 --> 00:03:29,465
You could have them on the one processor core and since they're actually alternating,

72
00:03:29,465 --> 00:03:31,150
like we saw in the last slide,

73
00:03:31,150 --> 00:03:34,315
you can execute them on one piece of hardware, one core,

74
00:03:34,315 --> 00:03:39,595
and you can still say this is concurrent execution but only one task executed at a time.

75
00:03:39,595 --> 00:03:43,120
So, this process of mapping

76
00:03:43,120 --> 00:03:47,110
the task to hardware is not directly controlled by the programmer.

77
00:03:47,110 --> 00:03:49,030
So, when I say mapping the task to the hardware,

78
00:03:49,030 --> 00:03:51,790
you've got these tasks, task one,

79
00:03:51,790 --> 00:03:54,405
task two, that you need to perform on some hardware,

80
00:03:54,405 --> 00:03:55,815
core one core two.

81
00:03:55,815 --> 00:04:00,795
And determining which core is performing which task,

82
00:04:00,795 --> 00:04:03,240
that's what I'm calling a hardware mapping.

83
00:04:03,240 --> 00:04:07,190
That is not directly controlled by the programmer,

84
00:04:07,190 --> 00:04:08,745
and certainly not in Go.

85
00:04:08,745 --> 00:04:12,675
Now there exists languages where the programmer directly controls that sort of thing.

86
00:04:12,675 --> 00:04:14,330
Those are generally considered,

87
00:04:14,330 --> 00:04:17,780
I'll call them exotic, they're not commonly used, put it like that.

88
00:04:17,780 --> 00:04:22,625
But that's a complicated thing and it's not part of Go,

89
00:04:22,625 --> 00:04:25,610
and it's not part of most languages either.

90
00:04:25,770 --> 00:04:28,220
So, most of the time,

91
00:04:28,220 --> 00:04:30,540
in a language like Go, when you program,

92
00:04:30,540 --> 00:04:31,800
when you do concurrent programming,

93
00:04:31,800 --> 00:04:34,000
the programmer in doing their concurrent programming,

94
00:04:34,000 --> 00:04:37,195
they're determining which tasks can be executed in parallel.

95
00:04:37,195 --> 00:04:39,385
Also, there terming a few other things like

96
00:04:39,385 --> 00:04:41,745
what kind of communication there is between the tasks,

97
00:04:41,745 --> 00:04:43,600
maybe one sends data to the other,

98
00:04:43,600 --> 00:04:45,310
are also synchronization events,

99
00:04:45,310 --> 00:04:47,095
we'll get into all these details later.

100
00:04:47,095 --> 00:04:49,520
But maybe, for instance,

101
00:04:49,520 --> 00:04:53,530
this test has to do some stuff before this test does it's other stuff,

102
00:04:53,530 --> 00:04:55,485
so they have to synchronize on a certain time.

103
00:04:55,485 --> 00:04:57,260
This must be finished before this time,

104
00:04:57,260 --> 00:05:00,000
this can finish after the time, something like that.

105
00:05:00,000 --> 00:05:02,640
We'll talk about that, but that's what the programmer does.

106
00:05:02,640 --> 00:05:06,995
A programmer describes what can be executed in parallel,

107
00:05:06,995 --> 00:05:09,345
not what will be executed in parallel.

108
00:05:09,345 --> 00:05:14,865
What will be executed in parallel depends on how the tasks are mapped to hardware.

109
00:05:14,865 --> 00:05:16,900
Now, how the tests are mapped to hardware,

110
00:05:16,900 --> 00:05:19,980
that is not directly in the control of the programmer,

111
00:05:19,980 --> 00:05:21,920
that's left to the operating system,

112
00:05:21,920 --> 00:05:27,150
also the Go run time scheduler that does part of the test two we'll talk about that too.

113
00:05:27,150 --> 00:05:29,650
That's part of the Go language actually,

114
00:05:29,650 --> 00:05:33,110
it gets compiled into your code and it handles some of that too.

115
00:05:33,110 --> 00:05:40,410
But concurrent programming is where the programmer defines the possible wconcurrency,

116
00:05:40,410 --> 00:05:42,890
What can be done in parallel, what can't be.

117
00:05:42,890 --> 00:05:47,650
But what actually is done in parallel and what is not depends on the mapping to hardware,

118
00:05:47,650 --> 00:05:51,500
and that's not under the control of the programmer.

119
00:05:51,810 --> 00:05:57,055
Now, let's say you are doing this, say you have one core.

120
00:05:57,055 --> 00:06:00,290
So you're doing concurrent programming but you've only got one core,

121
00:06:00,290 --> 00:06:02,755
so you can't actually get parallelism.

122
00:06:02,755 --> 00:06:06,010
So, one might say, look, why bother?

123
00:06:06,010 --> 00:06:08,100
Why go to all that effort of concurrent programming,

124
00:06:08,100 --> 00:06:10,295
if I'm only going to get

125
00:06:10,295 --> 00:06:13,080
concurrency anyway everything's happening one instruction at a time,

126
00:06:13,080 --> 00:06:16,140
these tasks can't actually run together at the same time.

127
00:06:16,140 --> 00:06:19,660
So, even though you can't get parallelism,

128
00:06:19,660 --> 00:06:23,090
maybe can't get parallelism cause you don't have concurrent multiple processes,

129
00:06:23,090 --> 00:06:25,145
multiple cores, you can still get

130
00:06:25,145 --> 00:06:28,460
significant performance improvement just from using concurrency.

131
00:06:28,460 --> 00:06:32,690
So, the reason why this happens is because typically,

132
00:06:32,690 --> 00:06:37,400
tasks have to periodically wait for some event, some slow event.

133
00:06:37,400 --> 00:06:39,900
Now, there's many of these slow events,

134
00:06:39,900 --> 00:06:41,810
typical input output things.

135
00:06:41,810 --> 00:06:45,120
So the idea is that in a machine there's a CPU,

136
00:06:45,120 --> 00:06:47,245
a processor, and that runs fast.

137
00:06:47,245 --> 00:06:50,320
But when the processor has to communicate with other things that

138
00:06:50,320 --> 00:06:54,880
are other chips on the same board, that is slow.

139
00:06:54,880 --> 00:06:57,400
So for instance say I got my processor and it needs to talk

140
00:06:57,400 --> 00:07:01,360
to some memory card that is far away on the board.

141
00:07:01,360 --> 00:07:02,830
When I say far away, I mean this far,

142
00:07:02,830 --> 00:07:06,065
but that's far, in terms of signal transmission, that's far.

143
00:07:06,065 --> 00:07:10,205
So he's got to communicate with a memory to grab some data from memory.

144
00:07:10,205 --> 00:07:13,985
And memories are slow so you typically have to wait for that.

145
00:07:13,985 --> 00:07:16,795
Maybe it wants to send data on the network,

146
00:07:16,795 --> 00:07:20,710
so it's got to communicate with this Ethernet card, or Wi-Fi card.

147
00:07:20,710 --> 00:07:22,115
And the Wi-Fi card is slow,

148
00:07:22,115 --> 00:07:25,930
it takes a certain amount of time to hand over the communication to receive or send.

149
00:07:25,930 --> 00:07:29,990
So anytime the CPU wants to do- or maybe you want to put something on screen,

150
00:07:29,990 --> 00:07:31,875
you've got to go to a video card.

151
00:07:31,875 --> 00:07:34,825
So the CPU is communicating with a video card and that has a certain delay.

152
00:07:34,825 --> 00:07:38,030
So when you have these IO activities where the CPU is communicating with

153
00:07:38,030 --> 00:07:41,005
something else on the board but separate from it,

154
00:07:41,005 --> 00:07:45,435
those are slow and often the code has to wait until those things are done.

155
00:07:45,435 --> 00:07:46,810
So for instance here,

156
00:07:46,810 --> 00:07:49,820
got this really simple instruction x equals y plus z.

157
00:07:49,820 --> 00:07:51,770
So in order to do this instruction,

158
00:07:51,770 --> 00:07:54,005
you've got to read y and z from memory,

159
00:07:54,005 --> 00:07:57,100
and then you can compute the addition

160
00:07:57,100 --> 00:08:00,835
and then you can put the result back in memory and x.

161
00:08:00,835 --> 00:08:04,745
These memory accesses reading y and z from memory can take

162
00:08:04,745 --> 00:08:09,410
over 100 clock cycles just waiting on memory to grab the data from memory.

163
00:08:09,410 --> 00:08:13,130
Now the add instruction itself takes you one clock cycle.

164
00:08:13,130 --> 00:08:17,540
But you might be waiting a 100 clock cycles just to read the data from memory.

165
00:08:17,540 --> 00:08:19,115
So when these type of delays,

166
00:08:19,115 --> 00:08:21,650
these pauses happen, they don't happen often.

167
00:08:21,650 --> 00:08:23,515
Hopefully, it depends on how your code's written.

168
00:08:23,515 --> 00:08:25,775
Hopefully it don't happen too often, but they can.

169
00:08:25,775 --> 00:08:28,960
Depending on how you access memory and memory access patterns and that sort of thing.

170
00:08:28,960 --> 00:08:32,040
When they happen, you would rather not have

171
00:08:32,040 --> 00:08:35,715
your processor just waiting there for 100 clock cycles, waiting for memory.

172
00:08:35,715 --> 00:08:40,810
It would be better if the task that's doing this x plus y, x equals y plus z,

173
00:08:40,810 --> 00:08:44,470
that task it has to wait for 100 cycles but in the meantime you would

174
00:08:44,470 --> 00:08:48,355
like to have that processor core executing some useful code.

175
00:08:48,355 --> 00:08:54,275
So if you have multiple tasks that all can execute when task 1 is waiting for memory,

176
00:08:54,275 --> 00:08:56,145
task 2 can execute,

177
00:08:56,145 --> 00:08:58,820
and then when task 2 maybe it hits a memory access,

178
00:08:58,820 --> 00:09:02,110
then task 3 can execute while the other guys are waiting.

179
00:09:02,110 --> 00:09:05,060
So that's called hiding latency.

180
00:09:05,060 --> 00:09:06,970
That's generally what you refer to with hiding latency.

181
00:09:06,970 --> 00:09:08,150
So there's a latency,

182
00:09:08,150 --> 00:09:09,890
like a wait time latency.

183
00:09:09,890 --> 00:09:12,755
So say I'm going to memory I need to wait 100 cycles.

184
00:09:12,755 --> 00:09:16,620
You can hide that latency by just taking another task that's ready

185
00:09:16,620 --> 00:09:20,720
to run and execute that while you're waiting on the first event.

186
00:09:20,720 --> 00:09:24,800
So even without having parallel hardware,

187
00:09:24,800 --> 00:09:26,480
having multiple core essay,

188
00:09:26,480 --> 00:09:30,800
just writing concurrent code can give you this significant advantage in performance

189
00:09:30,800 --> 00:09:35,210
because your program no longer has to wait on IO input-output accesses,

190
00:09:35,210 --> 00:09:36,825
whatever type of input-output accesses.

191
00:09:36,825 --> 00:09:39,775
No longer has to wait it can just run and our program's ready.

192
00:09:39,775 --> 00:09:41,930
And so things can still get done faster.

193
00:09:41,930 --> 00:09:46,810
So concurrent programming is useful even without parallel hardware.

194
00:09:46,910 --> 00:09:50,190
Now what I'm showing here is a picture that hardware mapping,

195
00:09:50,190 --> 00:09:53,070
I was talking about this hardware mapping concept.

196
00:09:53,070 --> 00:09:56,650
In the top I'm showing what parallel execution might look like,

197
00:09:56,650 --> 00:09:59,040
bottom I'm showing what concurrent execution might look like.

198
00:09:59,040 --> 00:10:01,925
So the idea here is, look at the parallel execution one first,

199
00:10:01,925 --> 00:10:03,660
you've got task 1, task 2.

200
00:10:03,660 --> 00:10:06,230
Imagine these are blocks of code written in whatever language,

201
00:10:06,230 --> 00:10:07,710
Go or whatever it is.

202
00:10:07,710 --> 00:10:10,540
And at the bottom you've got core 1 and core 2,

203
00:10:10,540 --> 00:10:12,770
so these are two cores in a 2-core system.

204
00:10:12,770 --> 00:10:14,920
This hardware mapping is a task of saying,

205
00:10:14,920 --> 00:10:16,570
this task runs on this core.

206
00:10:16,570 --> 00:10:18,645
So you can see with the parallel execution

207
00:10:18,645 --> 00:10:21,780
I'm mapping these two tasks to run on two different cores,

208
00:10:21,780 --> 00:10:26,260
task 1 and core 1, task 2 and core 2 so they can execute in parallel at the same time.

209
00:10:26,260 --> 00:10:29,325
In the bottom you do a different hardware mapping, you say,

210
00:10:29,325 --> 00:10:31,970
"Look I got two tasks and they are both performing,

211
00:10:31,970 --> 00:10:33,545
executing on the same core."

212
00:10:33,545 --> 00:10:35,565
That would be concurrent execution.

213
00:10:35,565 --> 00:10:41,435
But a key thing here is that this hardware mapping task from the task to the core,

214
00:10:41,435 --> 00:10:44,215
that is not directly under the control of

215
00:10:44,215 --> 00:10:47,685
the programmer in Go or any other substandard language.

216
00:10:47,685 --> 00:10:53,820
So, just to give an example why that's not- you might actually argue,

217
00:10:53,820 --> 00:10:54,895
"Well, look why not?

218
00:10:54,895 --> 00:10:56,720
Why not let the programmer do it?"

219
00:10:56,720 --> 00:10:57,980
That is a hard problem,

220
00:10:57,980 --> 00:11:02,665
it is a difficult problem and programmers generally don't want to do that.

221
00:11:02,665 --> 00:11:05,315
It would slow you down in writing your code.

222
00:11:05,315 --> 00:11:08,545
It makes your coding a lot harder if you actually perform the hardware mapping.

223
00:11:08,545 --> 00:11:10,770
There's a lot of research in doing

224
00:11:10,770 --> 00:11:13,130
this hardware mapping and automatic techniques for doing it,

225
00:11:13,130 --> 00:11:14,875
it's a hard task.

226
00:11:14,875 --> 00:11:20,005
The reason why is because these hardware architectures can be complicated and

227
00:11:20,005 --> 00:11:22,260
there are a lot of things that need to be considered when you're

228
00:11:22,260 --> 00:11:25,310
deciding which core is going to perform which task.

229
00:11:25,310 --> 00:11:27,110
And it would be

230
00:11:27,110 --> 00:11:31,505
a really big burden on the programmer to have to consider all these factors.

231
00:11:31,505 --> 00:11:33,575
And the program would have to be completely

232
00:11:33,575 --> 00:11:36,450
aware of the underlying hardware architecture.

233
00:11:36,450 --> 00:11:41,260
One good thing about traditional programming languages like Go Lang,

234
00:11:41,260 --> 00:11:42,970
but also Python C,

235
00:11:42,970 --> 00:11:46,230
whatever, is that when you're writing code in these languages you are

236
00:11:46,230 --> 00:11:49,795
not directly aware of the underlying hardware architecture.

237
00:11:49,795 --> 00:11:54,520
You don't know is this an I7 or is this an I3 or whatever.

238
00:11:54,520 --> 00:11:56,610
You don't know that, and you don't care.

239
00:11:56,610 --> 00:12:01,680
And you don't know exactly what the architecture looks like and you don't generally care.

240
00:12:01,680 --> 00:12:03,710
Now, to some extent you care a little.

241
00:12:03,710 --> 00:12:05,825
You might want to know, look I have at least this much memory.

242
00:12:05,825 --> 00:12:07,860
Maybe you care at a broad level.

243
00:12:07,860 --> 00:12:09,915
But the details of the hardware architecture,

244
00:12:09,915 --> 00:12:11,900
your coding is not dependent on that and

245
00:12:11,900 --> 00:12:13,975
you really don't want it to be because it is complicated.

246
00:12:13,975 --> 00:12:21,215
So what I've drawn here is a picture of a relatively simple arbitrary multi--core system.

247
00:12:21,215 --> 00:12:24,075
And this multi-core system has got these four cores,

248
00:12:24,075 --> 00:12:26,045
those blue cores around the edges.

249
00:12:26,045 --> 00:12:27,760
It's got four caches,

250
00:12:27,760 --> 00:12:30,380
local caches for each core which is extremely common.

251
00:12:30,380 --> 00:12:32,595
Actually this is a simplification of what you would expect to

252
00:12:32,595 --> 00:12:35,210
see but each one's got its local cache and then in

253
00:12:35,210 --> 00:12:38,105
the middle is a shared memory that all of them

254
00:12:38,105 --> 00:12:41,795
share and can read from and dump into the caches and so on.

255
00:12:41,795 --> 00:12:45,095
So when you're figuring out this hardware mapping saying,

256
00:12:45,095 --> 00:12:48,420
"Look, this task should be performed on this core this on this core."

257
00:12:48,420 --> 00:12:50,010
One big consideration is,

258
00:12:50,010 --> 00:12:51,505
where is the data?

259
00:12:51,505 --> 00:12:53,355
So what I mean by that is,

260
00:12:53,355 --> 00:12:56,880
if a particular core, core 1 let say.

261
00:12:56,880 --> 00:12:58,640
So core one is going to perform

262
00:12:58,640 --> 00:13:03,250
this particular task then it needs to access the data that the task uses.

263
00:13:03,250 --> 00:13:05,040
So where is the data located?

264
00:13:05,040 --> 00:13:08,200
The data, should you want it to be in the local cache for

265
00:13:08,200 --> 00:13:09,910
that core but say the data is in

266
00:13:09,910 --> 00:13:12,560
the shared memory or in the cache for another core or something like that.

267
00:13:12,560 --> 00:13:15,115
If the data is not where you want it,

268
00:13:15,115 --> 00:13:17,825
then if core 1- so for instance core 1's performing

269
00:13:17,825 --> 00:13:21,225
a task but all of it's data is on core 2's cache.

270
00:13:21,225 --> 00:13:24,185
Then, you've got to do these memory transfers.

271
00:13:24,185 --> 00:13:27,215
And remember the Von Neumann Bottleneck. Memory is slow.

272
00:13:27,215 --> 00:13:29,600
So doing transfers from memory to memory,

273
00:13:29,600 --> 00:13:32,175
during reads of memory can slow you down.

274
00:13:32,175 --> 00:13:35,065
So when you do this hardware mapping tests you've got to think,

275
00:13:35,065 --> 00:13:37,525
"Okay, where is the data?

276
00:13:37,525 --> 00:13:39,685
Which memories have the data and which don't?"

277
00:13:39,685 --> 00:13:44,035
And you basically want to put the task near the data.

278
00:13:44,035 --> 00:13:45,930
Also, what are the communication costs?

279
00:13:45,930 --> 00:13:50,545
So, maybe a cache to cache transfer is pretty fast because caches are fast,

280
00:13:50,545 --> 00:13:52,575
but if I could do cache to shared memory,

281
00:13:52,575 --> 00:13:53,820
shared memory is slower.

282
00:13:53,820 --> 00:13:59,280
So you would have to consider which communications are fast versus slow.

283
00:13:59,280 --> 00:14:02,420
So it can get to be a very complicated problem and I know that

284
00:14:02,420 --> 00:14:05,595
because I've seen plenty of research papers on that subject.

285
00:14:05,595 --> 00:14:07,485
It is a complicated problem it is not something

286
00:14:07,485 --> 00:14:10,025
that generally a programmer wants to have to consider.

287
00:14:10,025 --> 00:14:13,305
So it is intentionally left out of traditional languages like Go.

288
00:14:13,305 --> 00:14:16,140
So in Go we are doing concurrent programming so we

289
00:14:16,140 --> 00:14:20,190
can define which task can be performed in parallel,

290
00:14:20,190 --> 00:14:23,070
but which tasks are performed in parallel is left up

291
00:14:23,070 --> 00:14:27,190
to operating system Go Runtime something else.

292
00:14:27,190 --> 00:14:31,520
The programmer doesn't have to consider that. Thank you.